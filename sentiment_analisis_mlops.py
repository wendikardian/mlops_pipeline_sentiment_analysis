# -*- coding: utf-8 -*-
"""Sentiment_Analisis_MLOPs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CNx7vy1tSn5RzXG94zpv62k9JC_jcB9L

# Download Dataset dari kaggle
Sumber dataset : https://www.kaggle.com/datasets/dineshpiyasamara/sentiment-analysis-dataset
"""

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d dineshpiyasamara/sentiment-analysis-dataset

!mkdir data
!unzip sentiment-analysis-dataset.zip -d data
!ls data

"""#Install Library Requirements"""

!pip install jupyter scikit-learn tensorflow tfx==1.11.0 flask joblib

!pip install -U tfx

!pip uninstall shapely -y

"""# Check Tensorflow dan TFX apakah sudah terinstall dengan baik"""

import tensorflow as tf
print('TensorFlow version: {}'.format(tf.__version__))
from tfx import v1 as tfx
print('TFX version: {}'.format(tfx.__version__))

!pip install --upgrade protobuf

"""#Import Library yang akan digunakan"""

import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os

"""# Set Variable untuk Sistem Machine Learning Pipeline"""

PIPELINE_NAME = "sentiment-pipeline"
SCHEMA_PIPELINE_NAME = "sentiment-tfdv-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

# from absl import logging
# logging.set_verbosity(logging.INFO)

DATA_ROOT = "/content/data"

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""# Load dataset menggunakan ExampleGen
proses pengambilan data ke dalam ekosistem TFX (TensorFlow Extended) menggunakan komponen ExampleGen untuk persiapan data pada tahap preprocessing.
"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2)
    ])
)
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)

example_gen

interactive_context.run(example_gen)

# import sys
# import csv
# maxInt = sys.maxsize

# while True:
#     # decrease the maxInt value by factor 10
#     # as long as the OverflowError occurs.

#     try:
#         csv.field_size_limit(maxInt)
#         break
#     except OverflowError:
#         maxInt = int(maxInt/10)

"""# Membuat Summary Statistic pada dataset
 analisis statistik deskriptif seperti mean, median, dan distribusi variabel untuk memberikan wawasan komprehensif tentang karakteristik data
"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)
interactive_context.run(statistics_gen)

interactive_context.show(statistics_gen.outputs["statistics"])

"""# Data Schema TFX
identifikasi dan definisi struktur data, termasuk tipe data, keberadaan fitur, dan propertinya untuk memfasilitasi transformasi dan analisis data yang konsisten
"""

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"]
)
interactive_context.run(schema_gen)

interactive_context.show(schema_gen.outputs["schema"])

"""# Membuat Validator TFX
penggunaan komponen SchemaGen untuk menghasilkan skema data yang dapat diverifikasi, dan menentukan aturan validasi untuk memastikan kesesuaian data dengan skema yang telah ditentukan sebelumnya
"""

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)
interactive_context.run(example_validator)

interactive_context.show(example_validator.outputs['anomalies'])

"""# Preprocessing Data (Transform)
 penerapan transformasi pada dataset menggunakan komponen Transform untuk mengonversi dan mengubah fitur data berdasarkan aturan yang ditentukan sebelumnya, sehingga menghasilkan data yang siap untuk dilatih oleh model. Feature dan label yang telah ditransform menjadi tweet_tx, label_tx. Lalu tweet diubah kedalam format lowercase
"""

TRANSFORM_MODULE_FILE = "sentiment_transform.py"

# %%writefile {TRANSFORM_MODULE_FILE}
# import string
# import tensorflow as tf
# import tensorflow_transform as tft

# LABEL_KEY = "label"
# FEATURE_KEY = "tweet"


# def transformed_name(key):
#     return f"{key}_xf"


# def preprocessing_fn(inputs):
#     outputs = {}

#     text_sparse = inputs[FEATURE_KEY]

#     text_dense = tf.sparse.to_dense(text_sparse, default_value='')

#     text_lower = tf.strings.lower(text_dense)
#     non_empty_indices = tf.where(tf.strings.length(text_lower) > 0)

#     non_empty_values = tf.gather_nd(text_lower, non_empty_indices)
#     non_empty_indices = tf.cast(non_empty_indices, tf.int64)

#     sparse_tensor = tf.sparse.SparseTensor(
#         indices=non_empty_indices,
#         values=non_empty_values,
#         dense_shape=tf.shape(text_lower, out_type=tf.int64)
#     )
#     outputs[transformed_name(FEATURE_KEY)] = sparse_tensor
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)

#     return outputs

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# 
# import tensorflow as tf
# 
# LABEL_KEY = "label"
# FEATURE_KEY = "tweet"
# 
# def transformed_name(key) :
#     return key + "_xf"
# 
# def preprocessing_fn(inputs) :
#     outputs = {}
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
#     return outputs

transform = Transform(
    examples = example_gen.outputs["examples"],
    schema = schema_gen.outputs['schema'],
    module_file = os.path.abspath(TRANSFORM_MODULE_FILE)
)

interactive_context.run(transform)

"""# Tuner
mencari konfigurasi parameter terbaik dari model machine learning melalui eksperimen hyperparameter tuning

"""

TUNER_MODULE_FILE = "sentiment_tuner.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TUNER_MODULE_FILE}
# import keras_tuner as kt
# import tensorflow as tf
# import tensorflow_transform as tft
# from typing import NamedTuple, Dict, Text, Any
# from keras_tuner.engine import base_tuner
# from tensorflow.keras import layers
# from tfx.components.trainer.fn_args_utils import FnArgs
# from tensorflow.keras.callbacks import EarlyStopping
# 
# LABEL_KEY = "label"
# FEATURE_KEY = "tweet"
# NUM_EPOCHS = 2
# 
# TunerFnResult = NamedTuple("TunerFnResult", [
#     ("tuner", base_tuner.BaseTuner),
#     ("fit_kwargs", Dict[Text, Any]),
# ])
# 
# early_stopping_callback = EarlyStopping(
# monitor='binary_accuracy',  # Monitoring the binary accuracy
# patience=3,  # Number of epochs with no improvement after which training will be stopped
# min_delta=0.01,  # Minimum change in the monitored quantity to qualify as an improvement
# mode='max',  # Mode should be 'max' since we want to maximize the accuracy
# baseline=0.85  # Stop training once the accuracy reaches 85%
# )
# 
# def transformed_name(key):
#     return f"{key}_xf"
# 
# 
# def gzip_reader_fn(filenames):
#     return tf.data.TFRecordDataset(filenames, compression_type="GZIP")
# 
# 
# def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64):
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy()
#     )
# 
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key=transformed_name(LABEL_KEY),
#     )
# 
#     return dataset
# 
# 
# def model_builder(hp, vectorizer_layer):
#     num_hidden_layers = hp.Choice(
#         "num_hidden_layers", values=[1, 2]
#     )
#     embed_dims = hp.Int(
#         "embed_dims", min_value=16, max_value=128, step=32
#     )
#     lstm_units= hp.Int(
#         "lstm_units", min_value=32, max_value=128, step=32
#     )
#     dense_units = hp.Int(
#         "dense_units", min_value=32, max_value=256, step=32
#     )
#     dropout_rate = hp.Float(
#         "dropout_rate", min_value=0.1, max_value=0.5, step=0.1
#     )
#     learning_rate = hp.Choice(
#         "learning_rate", values=[1e-2, 1e-3, 1e-4]
#     )
# 
#     inputs = tf.keras.Input(
#         shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string
#     )
# 
#     x = vectorizer_layer(inputs)
#     x = layers.Embedding(input_dim=5000, output_dim=embed_dims)(x)
#     x = layers.Bidirectional(layers.LSTM(lstm_units))(x)
# 
#     for _ in range(num_hidden_layers):
#         x = layers.Dense(dense_units, activation=tf.nn.relu)(x)
#         x = layers.Dropout(dropout_rate)(x)
# 
#     outputs = layers.Dense(1, activation=tf.nn.sigmoid)(x)
# 
#     model = tf.keras.Model(inputs=inputs, outputs=outputs)
# 
#     model.compile(
#         optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
#         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#         metrics=["binary_accuracy"],
#     )
# 
#     return model
# 
# 
# def tuner_fn(fn_args: FnArgs):
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     train_set = input_fn(
#         fn_args.train_files[0], tf_transform_output, NUM_EPOCHS
#     )
#     eval_set = input_fn(
#         fn_args.eval_files[0], tf_transform_output, NUM_EPOCHS
#     )
# 
#     vectorizer_dataset = train_set.map(
#         lambda f, l: f[transformed_name(FEATURE_KEY)]
#     )
# 
#     vectorizer_layer = layers.TextVectorization(
#         max_tokens=5000,
#         output_mode="int",
#         output_sequence_length=500,
#     )
#     vectorizer_layer.adapt(vectorizer_dataset)
# 
#     tuner = kt.Hyperband(
#         hypermodel=lambda hp: model_builder(hp, vectorizer_layer),
#         objective=kt.Objective('binary_accuracy', direction='max'),
#         max_epochs=NUM_EPOCHS,
#         factor=3,
#         directory=fn_args.working_dir,
#         project_name="kt_hyperband",
#     )
# 
#     return TunerFnResult(
#         tuner=tuner,
#         fit_kwargs={
#             "callbacks": [early_stopping_callback],
#             "x": train_set,
#             "validation_data": eval_set,
#             "steps_per_epoch": fn_args.train_steps,
#             "validation_steps": fn_args.eval_steps,
#         },
#     )

from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2
tuner = Tuner(
    module_file=os.path.abspath(TUNER_MODULE_FILE),
    examples=transform.outputs["transformed_examples"],
    transform_graph=transform.outputs["transform_graph"],
    schema=schema_gen.outputs["schema"],
    train_args=trainer_pb2.TrainArgs(splits=["train"], num_steps=100),
    eval_args=trainer_pb2.EvalArgs(splits=["eval"], num_steps=50),
)
interactive_context.run(tuner)

"""# Trainning Model (Trainner)
melatih model machine learning menggunakan data yang telah dipreprocess, dengan mengoptimalkan parameter-model sesuai konfigurasi yang telah ditentukan.
"""

TRAINER_MODULE_FILE = "sentiment_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# LABEL_KEY = "label"
# FEATURE_KEY = "tweet"
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# 
# def input_fn(file_pattern,
#              tf_transform_output,
#              num_epochs,
#              batch_size=64)->tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = transformed_name(LABEL_KEY))
#     return dataset
# 
# # os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'
# # embed = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4")
# 
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE = 10000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH)
# 
# 
# embedding_dim=16
# def model_builder():
#     """Build machine learning model"""
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation='relu')(x)
#     x = layers.Dense(32, activation="relu")(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
# 
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
# 
#     )
# 
#     # print(model)
#     model.summary()
#     return model
# 
# 
# def _get_serve_tf_examples_fn(model, tf_transform_output):
# 
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
# 
#         feature_spec = tf_transform_output.raw_feature_spec()
# 
#         feature_spec.pop(LABEL_KEY)
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
# 
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs) -> None:
# 
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = log_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
# 
# 
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(FEATURE_KEY)]
#                 for i in list(train_set)]])
# 
#     # Build the model
#     model = model_builder()
# 
# 
#     # Train the model
#     model.fit(x = train_set,
#             validation_data = val_set,
#             callbacks = [tensorboard_callback, es, mc],
#             steps_per_epoch = 1000,
#             validation_steps= 1000,
#             epochs=10)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }
#     model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

from tfx.proto import trainer_pb2

trainer  = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)
interactive_context.run(trainer)

"""# Analisis dan Evaluasi Model (Resolver dan Evaluator)
menganalisis dan mengevaluasi kinerja model yang telah dilatih, memutuskan apakah model baru akan diadopsi berdasarkan metrik evaluasi yang ditentukan
"""

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    model_specs=[tfma.ModelSpec(label_key='label')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""# Export Model
menyimpanan model machine learning yang telah dilatih ke dalam format yang dapat digunakan di lingkungan produksi atau untuk inferensi
"""

from tfx.components import Transform, Trainer, Tuner, Evaluator, Pusher
from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2

pusher = Pusher(
    model=trainer.outputs["model"],
    model_blessing=evaluator.outputs["blessing"],
    push_destination=pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory=os.path.join(
                SERVING_MODEL_DIR, "sentiment-analisis-model"
            ),
        )
    )
)

interactive_context.run(pusher)

!zip -r /content/sentiment-pipeline.zip /content/pipelines

!zip -r /content/serving_model_dir.zip /content/serving_model/

!pip freeze >> requirements.txt